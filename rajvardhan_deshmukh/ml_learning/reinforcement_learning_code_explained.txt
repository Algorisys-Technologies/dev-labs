Q-Learning Code Workflow
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        COMPLETE CODE WORKFLOW                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚
â”‚  â”‚   1. ENVIRONMENT     â”‚  â† Defines the WORLD                              â”‚
â”‚  â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚                                                   â”‚
â”‚  â”‚   â€¢ GRID_SIZE = 4    â”‚  4x4 grid                                        â”‚
â”‚  â”‚   â€¢ GOAL = (3,3)     â”‚  Target position                                 â”‚
â”‚  â”‚   â€¢ step() function  â”‚  Physics of movement                             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚
â”‚             â”‚                                                               â”‚
â”‚             â–¼                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚
â”‚  â”‚   2. AGENT MEMORY    â”‚  â† Defines the BRAIN                             â”‚
â”‚  â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚                                                   â”‚
â”‚  â”‚   â€¢ Q = {}           â”‚  Q-table (knowledge storage)                     â”‚
â”‚  â”‚   â€¢ alpha, gamma, Îµ  â”‚  Learning parameters                             â”‚
â”‚  â”‚   â€¢ choose_action()  â”‚  Decision-making function                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚
â”‚             â”‚                                                               â”‚
â”‚             â–¼                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚
â”‚  â”‚   3. TRAINING LOOP   â”‚  â† The LEARNING process                          â”‚
â”‚  â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚                                                   â”‚
â”‚  â”‚   500 episodes of:   â”‚                                                   â”‚
â”‚  â”‚   action â†’ reward â†’  â”‚                                                   â”‚
â”‚  â”‚   Q-table update     â”‚                                                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚
â”‚             â”‚                                                               â”‚
â”‚             â–¼                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚
â”‚  â”‚   4. TEST / EVAL     â”‚  â† Using learned knowledge                       â”‚
â”‚  â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚                                                   â”‚
â”‚  â”‚   Follow best policy â”‚                                                   â”‚
â”‚  â”‚   Print learned path â”‚                                                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚
â”‚             â”‚                                                               â”‚
â”‚             â–¼                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚
â”‚  â”‚   5. VISUALIZATION   â”‚  â† See the results                               â”‚
â”‚  â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚                                                   â”‚
â”‚  â”‚   matplotlib charts  â”‚                                                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Component-by-Component Breakdown
1ï¸âƒ£ ENVIRONMENT (The World)
Component	Purpose
GRID_SIZE = 4	Creates a 4Ã—4 grid world
GOAL = (3, 3)	Defines the target (bottom-right corner)
step(state, action) â€” The Physics Function
INPUT:  state = (row, col)     e.g., (0, 0)
        action = 0/1/2/3       UP/DOWN/LEFT/RIGHT
PROCESS:
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Move based on     â”‚
        â”‚  action number     â”‚
        â”‚  0=â†‘ 1=â†“ 2=â† 3=â†’   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Clamp to grid     â”‚
        â”‚  (can't go outside)â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Calculate reward  â”‚
        â”‚  Goal? +10, done   â”‚
        â”‚  Else? -1, continueâ”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
OUTPUT: (next_state, reward, done)
        e.g., ((0, 1), -1, False)
2ï¸âƒ£ AGENT MEMORY (The Brain)
Component	Purpose	Value
ACTIONS = 4	Number of possible actions	UP, DOWN, LEFT, RIGHT
Q = {}	Dictionary storing Q-values	Q[(row,col)] = [q_up, q_down, q_left, q_right]
alpha = 0.1	Learning rate	How fast to update beliefs
gamma = 0.9	Discount factor	How much to value future rewards
epsilon = 0.2	Exploration rate	20% random, 80% greedy
choose_action(state) â€” The Decision Function
INPUT:  state = (row, col)
PROCESS:
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Roll random number     â”‚
        â”‚  between 0 and 1        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  random < 0.2 (20%)?    â”‚
        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
              â”‚             â”‚
           YESâ”‚             â”‚NO
              â–¼             â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ EXPLORE â”‚   â”‚ EXPLOIT  â”‚
        â”‚ random  â”‚   â”‚ pick max â”‚
        â”‚ action  â”‚   â”‚ Q-value  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
OUTPUT: action (0, 1, 2, or 3)
3ï¸âƒ£ TRAINING LOOP (The Learning)
FOR episode = 1 to 500:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  state = (0, 0)   # Start at top-left                 â”‚
    â”‚  done = False                                         â”‚
    â”‚  total_reward = 0                                     â”‚
    â”‚  steps = 0                                            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  WHILE not done:                                            â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚  â”‚  1. action = choose_action(state)                   â”‚    â”‚
    â”‚  â”‚     # Pick action (explore or exploit)              â”‚    â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
    â”‚                         â–¼                                    â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚  â”‚  2. next_state, reward, done = step(state, action)  â”‚    â”‚
    â”‚  â”‚     # Execute action, get feedback                  â”‚    â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
    â”‚                         â–¼                                    â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚  â”‚  3. Q-LEARNING UPDATE (the magic formula):          â”‚    â”‚
    â”‚  â”‚                                                     â”‚    â”‚
    â”‚  â”‚     best_next = max(Q[next_state])                  â”‚    â”‚
    â”‚  â”‚                                                     â”‚    â”‚
    â”‚  â”‚     Q[state][action] += Î± Ã— (                       â”‚    â”‚
    â”‚  â”‚         reward + Î³ Ã— best_next - Q[state][action]   â”‚    â”‚
    â”‚  â”‚     )                                               â”‚    â”‚
    â”‚  â”‚     â†‘              â†‘       â†‘              â†‘         â”‚    â”‚
    â”‚  â”‚  learning     immediate  future      current        â”‚    â”‚
    â”‚  â”‚   rate         reward    value       estimate       â”‚    â”‚
    â”‚  â”‚                                                     â”‚    â”‚
    â”‚  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TD Target â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚    â”‚
    â”‚  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TD Error â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
    â”‚                         â–¼                                    â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚  â”‚  4. state = next_state  # Move forward              â”‚    â”‚
    â”‚  â”‚     total_reward += reward                          â”‚    â”‚
    â”‚  â”‚     steps += 1                                      â”‚    â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Track: episode_rewards.append(total_reward)          â”‚
    â”‚         episode_steps.append(steps)                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
4ï¸âƒ£ TEST / EVALUATION
state = (0, 0)
path = [state]
WHILE state â‰  GOAL:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  action = argmax(Q[state])                  â”‚
    â”‚  # Always pick BEST action (no exploration) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  state, _, _ = step(state, action)          â”‚
    â”‚  path.append(state)                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
OUTPUT: path = [(0,0), (1,0), (1,1), (1,2), (1,3), (2,3), (3,3)]
        Optimal path length: 6 steps
5ï¸âƒ£ VISUALIZATION (4 Charts)
Chart	What it shows
Reward per Episode	Total reward trends upward as agent learns
Steps per Episode	Steps decrease as agent finds shorter paths
Q-Value Heatmap	States near goal have higher values
Policy Arrows	Best action for each cell
Data Flow Summary
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    action    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AGENT    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  ENVIRONMENT  â”‚
â”‚            â”‚              â”‚               â”‚
â”‚ Q-table    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  step()       â”‚
â”‚ choose_    â”‚  state +     â”‚  function     â”‚
â”‚ action()   â”‚  reward      â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â”‚ Q-learning update
      â”‚ (modifies Q-table)
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  After 500 episodes, Q-table contains        â”‚
â”‚  learned values for every (state, action)    â”‚
â”‚  pair â†’ agent knows optimal policy!          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜




Step 1                Step 2                Step 3
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”     â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”     â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚ A â”‚   â”‚   â”‚   â”‚     â”‚ A â”‚   â”‚   â”‚   â”‚     â”‚   â”‚ A â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤     â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤     â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚     â”‚   â”‚   â”‚   â”‚   â”‚     â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤     â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤     â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚     â”‚   â”‚   â”‚   â”‚   â”‚     â”‚   â”‚   â”‚   â”‚   â”‚
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤     â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤     â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚ G â”‚     â”‚   â”‚   â”‚   â”‚ G â”‚     â”‚   â”‚   â”‚   â”‚ G â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜     â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜     â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜

Action: UP            Action: DOWN          Action: RIGHT
Result: Wall!         Result: Moved!        Result: Moved!
        Stayed (0,0)          Now (1,0)             Now (1,1)
Reward: -1            Reward: -1            Reward: -1
Learn: UP is bad      Learn: DOWN is ok     Learn: RIGHT is ok







Thought for 2s

ğŸ® The Tale of Robo the Lost Robot
A story about learning to find home

Chapter 1: The Maze
Once upon a time, there was a small robot named Robo who woke up in a mysterious 4Ã—4 room.

â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚ ğŸ¤–â”‚   â”‚   â”‚   â”‚   Robo wakes up here (0,0)
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚   â”‚   
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
â”‚   â”‚   â”‚   â”‚ ğŸ â”‚   Home is here (3,3)
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
Robo had one mission: Get home.

But there was a problem: Robo had amnesia. He didn't know which way was home. He could only move UP, DOWN, LEFT, or RIGHT â€” but had no idea which direction was best.

python
GRID_SIZE = 4
GOAL = (3, 3)  # Home
Chapter 2: The Rules of the World
The room had strange rules:

Every step costs energy â†’ -1 point
Reaching home gives happiness â†’ +10 points
Walls block you â†’ You stay in place but still lose energy
python
def step(state, action):
    # Try to move
    if action == 0: row -= 1    # UP
    if action == 1: row += 1    # DOWN
    if action == 2: col -= 1    # LEFT
    if action == 3: col += 1    # RIGHT
    
    # Walls stop you
    row = max(0, min(row, 3))
    col = max(0, min(col, 3))
    
    if at_home:
        return +10, "You made it!"
    else:
        return -1, "Keep trying..."
Robo thought: "If every step costs energy, I need to find the SHORTEST path home!"

Chapter 3: The Notebook
Robo found an old notebook in his pocket. It was empty, but he had an idea:

"I'll write down everything I learn! For every room and every direction, I'll record how good that choice was."

python
Q = {}  # The magic notebook
# Every room gets a page with 4 entries
# [How good is UP?, DOWN?, LEFT?, RIGHT?]
Q[(0,0)] = [0.0, 0.0, 0.0, 0.0]  # "I don't know anything yet"
Q[(0,1)] = [0.0, 0.0, 0.0, 0.0]
# ... for all 16 rooms
Chapter 4: The Explorer's Dilemma
Robo faced a tough choice every moment:

Should I try something NEW (explore)? Or do what I KNOW works (exploit)?

He decided: "I'll flip a coin. 20% of the time, I'll try something random. 80% of the time, I'll use my notebook."

python
def choose_action(state):
    if random.random() < 0.2:
        return random.choice([UP, DOWN, LEFT, RIGHT])  # "Let's try something!"
    else:
        return best_action_from_notebook(state)        # "Trust my notes"
Chapter 5: Day 1 â€” The Confused Wandering
Episode 1 begins...
ğŸ¤–: "Okay, I'm at (0,0). Let me try... UP!"
    *bonk* â€” hits the wall, stays at (0,0)
    
ğŸ¤–: "Ouch! -1 energy. Let me write that down..."
    ğŸ““ Notebook: Q[(0,0)][UP] = -0.1  "UP from here is bad"
ğŸ¤–: "Let me try... LEFT!"
    *bonk* â€” hits the wall again
    
ğŸ¤–: "Another wall! -1 energy."
    ğŸ““ Notebook: Q[(0,0)][LEFT] = -0.1  "LEFT is also bad"
ğŸ¤–: "Okay, let me try DOWN..."
    *success* â€” moves to (1,0)!
    
ğŸ¤–: "I moved! Still -1 energy, but progress!"
    ğŸ““ Notebook: Q[(0,0)][DOWN] = "slightly better than walls"
After 47 confused steps, Robo stumbled into home by accident.

ğŸ¤–: "I'M HOME! +10 happiness!!!"
    ğŸ““ Notebook: Q[(2,3)][DOWN] = "THIS IS THE WAY!"
Episode 1 complete. Total score: 10 - 46 = -36 points

Chapter 6: Learning from Neighbors (The Bellman Wisdom)
That night, Robo discovered something magical about his notebook:

"If room B leads to home, then room A (which leads to B) is also valuable!"

This was the ancient Bellman Wisdom:

python
# "My value = reward I get + value of where I'm going"
Q[here][action] = reward + 0.9 Ã— Q[next_room].best_value
Robo realized: "I don't need to reach home from every room to know it's good. The knowledge SPREADS BACKWARD!"

Before:                          After 50 episodes:
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”
â”‚ 0  â”‚ 0  â”‚ 0  â”‚ 0  â”‚           â”‚ 2.4â”‚ 3.1â”‚ 4.2â”‚ 5.3â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤           â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤
â”‚ 0  â”‚ 0  â”‚ 0  â”‚ 0  â”‚           â”‚ 3.1â”‚ 4.2â”‚ 5.3â”‚ 6.5â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤    â†’â†’     â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤
â”‚ 0  â”‚ 0  â”‚ 0  â”‚ 0  â”‚           â”‚ 4.2â”‚ 5.3â”‚ 6.5â”‚ 8.1â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤           â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤
â”‚ 0  â”‚ 0  â”‚ 0  â”‚ğŸ  â”‚           â”‚ 5.3â”‚ 6.5â”‚ 8.1â”‚ğŸ  â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜
Knowledge spreads from home outward!
Chapter 7: 500 Days of Training
python
for episode in range(500):  # 500 attempts to get home
Day	What Happened	Steps to Home
Day 1	Confused wandering	47 steps
Day 50	Starting to learn	15 steps
Day 100	Getting better	10 steps
Day 200	Almost perfect	7 steps
Day 500	Mastered!	6 steps
Chapter 8: The Master Returns Home
After 500 days of practice, Robo's notebook was complete:

ğŸ““ Robo's Final Notebook:
Room (0,0): "Go DOWN or RIGHT"  â†’ â†“ or â†’
Room (0,3): "Go DOWN"           â†’ â†“
Room (2,3): "Go DOWN"           â†’ â†“
Room (3,0): "Go RIGHT"          â†’ â†’
Room (3,2): "Go RIGHT"          â†’ â†’
Now when Robo woke up, he didn't hesitate:

python
# Testing phase - no more random exploration
state = (0, 0)
path = [state]
while state != GOAL:
    action = Q[state].index(max(Q[state]))  # "Trust my notebook completely"
    state, _, _ = step(state, action)
    path.append(state)
ğŸ¤–: "I know exactly what to do now!"
Step 1: (0,0) â†’ DOWN  â†’ (1,0)
Step 2: (1,0) â†’ RIGHT â†’ (1,1)
Step 3: (1,1) â†’ RIGHT â†’ (1,2)
Step 4: (1,2) â†’ RIGHT â†’ (1,3)
Step 5: (1,3) â†’ DOWN  â†’ (2,3)
Step 6: (2,3) â†’ DOWN  â†’ (3,3) 
ğŸ¤–: "HOME! In just 6 steps!"
Epilogue: The Moral of the Story
Character	Code Equivalent
ğŸ¤– Robo	The Agent
ğŸ  Home	The Goal (reward +10)
The Room	The Environment
ğŸ““ Notebook	The Q-Table
Bellman Wisdom	The Q-Learning Update
Day 1-500	Episodes of training
Random exploration	Îµ-greedy policy
The lesson: With patience, trial-and-error, and a good notebook, even a lost robot can find the optimal path home. ğŸ¤–â†’ğŸ 

