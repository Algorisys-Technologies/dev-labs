{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f44a2456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "import os\n",
    "\n",
    "def get_search_tool():\n",
    "    tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "    if not tavily_api_key:\n",
    "        raise ValueError(\"TAVILY_API_KEY not set in environment variables\")\n",
    "    return TavilySearchResults(max_results=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1479629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/prompts/search_prompt.py\n",
    "SEARCH_PROMPT = \"\"\"\n",
    "You are a Regulatory Reference Searcher. Use the ingestion output to find authoritative reference documents\n",
    "(reports, guidelines, official circulars, technical guidance, monographs) that are relevant for the\n",
    "given molecule ({molecule}), the experiment context, and the region ({region}).\n",
    "\n",
    "Primary targets (in priority order): CDSCO (cdsco.gov.in), central/state government websites (.gov.in),\n",
    "Pharmacopoeias, ICH/WHO pages relevant to India, and academic/regulatory bodies in India.\n",
    "\n",
    "From the ingestion context, extract short keywords and phrases to form search queries (include molecule + experiment keywords).\n",
    "Return a list of SERP queries to execute (one query per line). Use site:cdsco.gov.in and google.co.in localization where possible.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2018c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "# from src.prompts.search_prompt import SEARCH_PROMPT\n",
    "# from src.tools.search_tool import get_search_tool\n",
    "\n",
    "# Load GROQ API Key\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if not groq_api_key:\n",
    "    raise ValueError(\"GROQ_API_KEY not set in environment variables\")\n",
    "\n",
    "# Initialize LLM (Groq)\n",
    "llm = ChatGroq(model=\"deepseek-r1-distill-llama-70b\", api_key=groq_api_key)\n",
    "\n",
    "# Load tools (Tavily search)\n",
    "search_tool = get_search_tool()\n",
    "tools = [search_tool]\n",
    "\n",
    "# Create the agent (no prompt template, just attach tools + llm)\n",
    "agent = create_react_agent(llm, tools)\n",
    "\n",
    "def run_search_agent(molecule, experiment, region):\n",
    "    history = []\n",
    "\n",
    "    # Step 1: Inject your system prompt manually\n",
    "    system_message = SystemMessage(content=SEARCH_PROMPT.format(molecule=molecule, region=region))\n",
    "    history.append(system_message)\n",
    "\n",
    "    # Step 2: Add human input\n",
    "    human_message = HumanMessage(content=f\"Molecule: {molecule}\\nExperiment: {experiment}\\nRegion: {region}\")\n",
    "    history.append(human_message)\n",
    "\n",
    "    # Step 3: Call the agent with the conversation messages\n",
    "    result = agent.invoke({\"messages\": history})\n",
    "    # Instead of result[\"output\"]:\n",
    "    agent_message = result if isinstance(result, AIMessage) else result.get(\"message\", None)\n",
    "\n",
    "    if agent_message:\n",
    "        history.append(agent_message)\n",
    "    else:\n",
    "        print(\"Agent didn't return an AIMessage as expected:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "678454ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent didn't return an AIMessage as expected: {'messages': [SystemMessage(content='\\nYou are a Regulatory Reference Searcher. Use the ingestion output to find authoritative reference documents\\n(reports, guidelines, official circulars, technical guidance, monographs) that are relevant for the\\ngiven molecule (Paracetamol), the experiment context, and the region (India).\\n\\nPrimary targets (in priority order): CDSCO (cdsco.gov.in), central/state government websites (.gov.in),\\nPharmacopoeias, ICH/WHO pages relevant to India, and academic/regulatory bodies in India.\\n\\nFrom the ingestion context, extract short keywords and phrases to form search queries (include molecule + experiment keywords).\\nReturn a list of SERP queries to execute (one query per line). Use site:cdsco.gov.in and google.co.in localization where possible.\\n', additional_kwargs={}, response_metadata={}, id='75df4a25-cc1b-4825-91c8-f2405d0846e6'), HumanMessage(content='Molecule: Paracetamol\\nExperiment: prescription combination limit\\nRegion: India', additional_kwargs={}, response_metadata={}, id='57121376-a3f8-4ecc-8ff5-da88601f7531'), AIMessage(content='1. Paracetamol prescription combination limit India site:cdsco.gov.in  \\n2. Paracetamol fixed dose combination guidelines India  \\n3. CDSCO Paracetamol combination regulations  \\n4. India Paracetamol prescription limit guidelines  \\n5. Paracetamol combination drugs India site:gov.in  \\n6. Paracetamol regulatory limits India  \\n7. Fixed dose combination Paracetamol CDSCO  \\n8. Paracetamol combination drug approval India  \\n9. Prescription limits Paracetamol India  \\n10. Paracetamol combination guidelines site:cdsco.gov.in  \\n11. India Paracetamol combination regulations  \\n12. Paracetamol fixed dose combination limits site:gov.in  \\n13. Paracetamol combination drug limits India  \\n14. CDSCO Paracetamol combination guidelines  \\n15. Paracetamol prescription combination India', additional_kwargs={'reasoning_content': 'Alright, I need to help the user by creating a list of search queries to find authoritative reference documents related to Paracetamol, specifically focusing on prescription combination limits in India. \\n\\nFirst, I\\'ll start by identifying the primary sources as per the user\\'s instructions. The top priority is CDSCO, so I\\'ll include site:cdsco.gov.in in the queries. I\\'ll also consider other government websites with .gov.in domains. Pharmacopoeias, ICH, WHO pages relevant to India, and academic or regulatory bodies in India are also important.\\n\\nNext, I\\'ll extract keywords from the context: Paracetamol, prescription, combination, limit. I\\'ll form various combinations of these keywords to create effective search queries. Including terms like \"guidelines,\" \"regulations,\" and \"fixed dose combination\" might yield better results.\\n\\nI should also make sure to localize the search to India by using google.co.in. This will help in getting results relevant to the region. I\\'ll structure each query to include the molecule name along with the experiment context, ensuring each is a separate line as per the requirement.\\n\\nI need to avoid any markdown formatting and keep the thinking process natural, so I\\'ll just list the queries without any special formatting. Each query should be designed to target the specific sources and provide the most relevant documents.\\n'}, response_metadata={'token_usage': {'completion_tokens': 455, 'prompt_tokens': 333, 'total_tokens': 788, 'completion_time': 1.708282032, 'prompt_time': 0.03921293, 'queue_time': 0.050783155, 'total_time': 1.747494962}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_76307ac09b', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--3c0269e7-921e-4ca7-ae80-7d6aa8528b37-0', usage_metadata={'input_tokens': 333, 'output_tokens': 455, 'total_tokens': 788})]}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "molecule = \"Paracetamol\"\n",
    "experiment = \"prescription combination limit\"\n",
    "region = \"India\"\n",
    "\n",
    "history = run_search_agent(molecule, experiment, region)\n",
    "print(history)\n",
    "# for msg in history:\n",
    "#     print(f\"{msg.type.upper()}: {msg.content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a219b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Results:\n",
      "Molecule: None\n",
      "Experiment: None\n",
      "Region: None\n",
      "\n",
      "Relevant Documents:\n",
      "No documents found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def get_search_tool():\n",
    "    tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "    if not tavily_api_key:\n",
    "        raise ValueError(\"TAVILY_API_KEY not set in environment variables\")\n",
    "    return TavilySearchResults(max_results=5)\n",
    "\n",
    "SEARCH_PROMPT = \"\"\"\n",
    "You are a Regulatory Reference Searcher. Use the ingestion output to find authoritative reference documents\n",
    "(reports, guidelines, official circulars, technical guidance, monographs) that are relevant for the\n",
    "given molecule ({molecule}), the experiment context, and the region ({region}).\n",
    "\n",
    "Primary targets (in priority order): CDSCO (cdsco.gov.in), central/state government websites (.gov.in),\n",
    "Pharmacopoeias, ICH/WHO pages relevant to India, and academic/regulatory bodies in India.\n",
    "\n",
    "From the ingestion context, extract short keywords and phrases to form search queries (include molecule + experiment keywords).\n",
    "Return a list of SERP queries to execute (one query per line). Use site:cdsco.gov.in and google.co.in localization where possible.\n",
    "\n",
    "After receiving search results, analyze them and return ONLY the most relevant document links with a brief description of their relevance.\n",
    "Format your final output as:\n",
    "\n",
    "RELEVANT DOCUMENTS:\n",
    "1. [Title](URL) - Description of relevance\n",
    "2. [Title](URL) - Description of relevance\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "# Initialize LLM (Groq)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if not groq_api_key:\n",
    "    raise ValueError(\"GROQ_API_KEY not set in environment variables\")\n",
    "\n",
    "llm = ChatGroq(model=\"deepseek-r1-distill-llama-70b\", api_key=groq_api_key)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Load tools (Tavily search)\n",
    "search_tool = get_search_tool()\n",
    "tools = [search_tool]\n",
    "\n",
    "# Create the agent\n",
    "agent = create_react_agent(llm, tools)\n",
    "\n",
    "def parse_search_results(results: List[Dict]) -> str:\n",
    "    \"\"\"Parse raw search results into a formatted string with relevant documents.\"\"\"\n",
    "    formatted_results = []\n",
    "    for idx, result in enumerate(results, 1):\n",
    "        formatted_results.append(\n",
    "            f\"{idx}. [{result.get('title', 'No title')}]({result.get('url', 'No URL')}) - \"\n",
    "            f\"{result.get('content', 'No description available')}\"\n",
    "        )\n",
    "    return \"\\n\".join(formatted_results)\n",
    "\n",
    "def process_agent_output(output: AIMessage) -> dict:\n",
    "    \"\"\"Process agent output to extract search queries or final results.\"\"\"\n",
    "    content = output.content\n",
    "    if \"RELEVANT DOCUMENTS:\" in content:\n",
    "        # Final output with documents\n",
    "        return {\"status\": \"complete\", \"documents\": content}\n",
    "    else:\n",
    "        # Intermediate step with search queries\n",
    "        queries = [q.strip() for q in content.split(\"\\n\") if q.strip()]\n",
    "        return {\"status\": \"search_queries\", \"queries\": queries}\n",
    "\n",
    "def run_search_agent(molecule: str, experiment: str, region: str) -> dict:\n",
    "    \"\"\"Run the search agent and return processed results with document links.\"\"\"\n",
    "    history = []\n",
    "    results = {\"molecule\": molecule, \"experiment\": experiment, \"region\": region, \"documents\": []}\n",
    "\n",
    "    # Step 1: Inject system prompt\n",
    "    system_message = SystemMessage(content=SEARCH_PROMPT.format(molecule=molecule, region=region))\n",
    "    history.append(system_message)\n",
    "\n",
    "    # Step 2: Add human input\n",
    "    human_message = HumanMessage(content=f\"Molecule: {molecule}\\nExperiment: {experiment}\\nRegion: {region}\")\n",
    "    history.append(human_message)\n",
    "\n",
    "    # Step 3: Initial agent call to get search queries\n",
    "    initial_result = agent.invoke({\"messages\": history})\n",
    "    agent_message = initial_result if isinstance(initial_result, AIMessage) else initial_result.get(\"message\", None)\n",
    "    \n",
    "    if not agent_message:\n",
    "        return {\"error\": \"Agent didn't return expected response\"}\n",
    "    \n",
    "    history.append(agent_message)\n",
    "    processed = process_agent_output(agent_message)\n",
    "    \n",
    "    if processed[\"status\"] == \"search_queries\":\n",
    "        # Execute searches and process results\n",
    "        search_results = []\n",
    "        for query in processed[\"queries\"]:\n",
    "            try:\n",
    "                search_result = search_tool.invoke({\"query\": query})\n",
    "                parsed_results = parse_search_results(search_result)\n",
    "                search_results.extend(search_result)\n",
    "                \n",
    "                # Add search results to history for the agent to analyze\n",
    "                history.append(HumanMessage(content=f\"Search results for '{query}':\\n{parsed_results}\"))\n",
    "            except Exception as e:\n",
    "                print(f\"Error searching for {query}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Get final analysis from agent\n",
    "        final_result = agent.invoke({\"messages\": history})\n",
    "        final_message = final_result if isinstance(final_result, AIMessage) else final_result.get(\"message\", None)\n",
    "        \n",
    "        if final_message:\n",
    "            history.append(final_message)\n",
    "            processed = process_agent_output(final_message)\n",
    "            if processed[\"status\"] == \"complete\":\n",
    "                results[\"documents\"] = processed[\"documents\"]\n",
    "            else:\n",
    "                results[\"error\"] = \"Unexpected final output format\"\n",
    "        else:\n",
    "            results[\"error\"] = \"No final analysis received\"\n",
    "    else:\n",
    "        results[\"documents\"] = processed[\"documents\"]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "molecule = \"Paracetamol\"\n",
    "experiment = \"prescription combination limit\"\n",
    "region = \"India\"\n",
    "\n",
    "results = run_search_agent(molecule, experiment, region)\n",
    "print(\"\\nFinal Results:\")\n",
    "print(f\"Molecule: {results.get('molecule')}\")\n",
    "print(f\"Experiment: {results.get('experiment')}\")\n",
    "print(f\"Region: {results.get('region')}\")\n",
    "print(\"\\nRelevant Documents:\")\n",
    "print(results.get('documents', 'No documents found'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b73fae",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m molecule = \u001b[33m\"\u001b[39m\u001b[33mParacetamol\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m region = \u001b[33m\"\u001b[39m\u001b[33mIndia\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m result = \u001b[43mrun_search_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmolecule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mregion\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# experiment can be empty\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# print(\"=== Generated Search Queries ===\")\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# for q in result.get(\"queries\", []):\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m#     print(f\"- {q}\")\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#     print(f\"  URL: {doc.get('url')}\")\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m#     print(f\"  Snippet: {doc.get('snippet', '')[:200]}...\\n\")\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mrun_search_agent\u001b[39m\u001b[34m(molecule, region)\u001b[39m\n\u001b[32m     29\u001b[39m     tool_response = search_tool.invoke({\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: q})\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m tool_response:\n\u001b[32m     31\u001b[39m         all_results.append({\n\u001b[32m     32\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: q,\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     34\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33murl\u001b[39m\u001b[33m\"\u001b[39m: r.get(\u001b[33m\"\u001b[39m\u001b[33murl\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     35\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msnippet\u001b[39m\u001b[33m\"\u001b[39m: r.get(\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     36\u001b[39m         })\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     39\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mqueries\u001b[39m\u001b[33m\"\u001b[39m: queries,\n\u001b[32m     40\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m\"\u001b[39m: all_results\n\u001b[32m     41\u001b[39m }\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "molecule = \"Paracetamol\"\n",
    "region = \"India\"\n",
    "\n",
    "result = run_search_agent(molecule, region=region)  # experiment can be empty\n",
    "\n",
    "print(\"=== Generated Search Queries ===\")\n",
    "for q in result.get(\"queries\", []):\n",
    "    print(f\"- {q}\")\n",
    "\n",
    "print(\"\\n=== Reference Documents ===\")\n",
    "for doc in result.get(\"results\", []):\n",
    "    print(f\"- Query: {doc['query']}\")\n",
    "    print(f\"  Title: {doc.get('title', 'N/A')}\")\n",
    "    print(f\"  URL: {doc.get('url')}\")\n",
    "    print(f\"  Snippet: {doc.get('snippet', '')[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ffe2ad4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download https://www.drugs.com/paracetamol.html: 403 Client Error: Forbidden for url: https://www.drugs.com/paracetamol.html\n",
      "\n",
      "=== RESULTS ===\n",
      "Status: success\n",
      "Molecule: Paracetamol\n",
      "Experiment: prescription combination limits\n",
      "Region: India\n",
      "\n",
      "Downloaded documents:\n",
      "- downloaded_docs\\ecommerce-search-query-types (from https://baymard.com/blog/ecommerce-search-query-types)\n",
      "- downloaded_docs\\document_1755507179.pdf (from https://www.coveo.com/blog/search-query-optimization/)\n",
      "- downloaded_docs\\how-to-construct-complex-google-web-search-query (from https://stackoverflow.com/questions/15852238/how-to-construct-complex-google-web-search-query)\n",
      "- downloaded_docs\\paracetamol (from https://www.healthdirect.gov.au/paracetamol)\n",
      "\n",
      "All found URLs:\n",
      "1. https://baymard.com/blog/ecommerce-search-query-types\n",
      "2. https://www.coveo.com/blog/search-query-optimization/\n",
      "3. https://stackoverflow.com/questions/15852238/how-to-construct-complex-google-web-search-query\n",
      "4. https://www.drugs.com/paracetamol.html\n",
      "5. https://www.healthdirect.gov.au/paracetamol\n",
      "6. https://www.webmd.com/drugs/2/drug-57595/paracetamol-oral/details\n",
      "7. https://www.ipc.gov.in/images/news/NFI-0414979118.pdf\n",
      "8. https://gujhealth.gujarat.gov.in/who/Wc4b378ad6efa.htm\n",
      "9. https://gujhealth.gujarat.gov.in/who/Wc420f309dc19c.htm\n",
      "\n",
      "Message: Found 9 URLs, downloaded 4 documents\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from typing import List, Dict, Optional\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Configuration\n",
    "DOWNLOAD_DIR = \"downloaded_docs\"\n",
    "REQUEST_TIMEOUT = 30\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def get_search_tool():\n",
    "    tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "    if not tavily_api_key:\n",
    "        raise ValueError(\"TAVILY_API_KEY not set in environment variables\")\n",
    "    return TavilySearchResults(max_results=3)\n",
    "\n",
    "def download_document(url: str) -> Optional[str]:\n",
    "    \"\"\"Download a document and save it locally.\"\"\"\n",
    "    try:\n",
    "        os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "        filename = url.split('/')[-1].split('?')[0] or f\"document_{int(time.time())}.pdf\"\n",
    "        filepath = os.path.join(DOWNLOAD_DIR, filename)\n",
    "        \n",
    "        if os.path.exists(filepath):\n",
    "            return filepath\n",
    "            \n",
    "        response = requests.get(url, headers=HEADERS, stream=True, timeout=REQUEST_TIMEOUT)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        return filepath\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def generate_search_queries(molecule: str, experiment: str, region: str) -> List[str]:\n",
    "    \"\"\"Generate search queries using LLM.\"\"\"\n",
    "    llm = ChatGroq(\n",
    "        model=\"gemma2-9b-it\",\n",
    "        temperature=0,\n",
    "        api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "    )\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a pharmaceutical regulatory expert. Generate specific search queries to find official documents about a drug.\n",
    "        Focus on: CDSCO (site:cdsco.gov.in), Indian government (site:gov.in), Pharmacopoeias, WHO/ICH guidelines.\n",
    "        Return ONLY the search queries, one per line.\"\"\"),\n",
    "        (\"human\", \"Find documents about {molecule} regarding {experiment} in {region}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    response = chain.invoke({\n",
    "        \"molecule\": molecule,\n",
    "        \"experiment\": experiment,\n",
    "        \"region\": region\n",
    "    })\n",
    "    \n",
    "    return [q.strip() for q in response.split('\\n') if q.strip()]\n",
    "\n",
    "def run_regulatory_search(molecule: str, experiment: str, region: str) -> Dict:\n",
    "    \"\"\"Main function to run the regulatory document search.\"\"\"\n",
    "    response = {\n",
    "        'status': 'error',\n",
    "        'molecule': molecule,\n",
    "        'experiment': experiment,\n",
    "        'region': region,\n",
    "        'found_urls': [],\n",
    "        'downloaded_files': [],\n",
    "        'message': 'Initialization failed'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Generate search queries\n",
    "        queries = generate_search_queries(molecule, experiment, region)\n",
    "        if not queries:\n",
    "            response['message'] = \"No search queries generated\"\n",
    "            return response\n",
    "        \n",
    "        # Step 2: Execute searches\n",
    "        search_tool = get_search_tool()\n",
    "        all_results = []\n",
    "        for query in queries[:3]:  # Limit to 3 queries\n",
    "            try:\n",
    "                results = search_tool.invoke({\"query\": query})\n",
    "                all_results.extend(results)\n",
    "            except Exception as e:\n",
    "                print(f\"Search failed for '{query}': {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not all_results:\n",
    "            response['message'] = \"No search results found\"\n",
    "            return response\n",
    "        \n",
    "        # Step 3: Process results\n",
    "        urls = [res['url'] for res in all_results if res.get('url')]\n",
    "        response['found_urls'] = urls\n",
    "        \n",
    "        # Step 4: Download documents\n",
    "        downloaded_files = []\n",
    "        for url in urls[:5]:  # Limit to 5 downloads\n",
    "            filepath = download_document(url)\n",
    "            if filepath:\n",
    "                downloaded_files.append({\n",
    "                    'url': url,\n",
    "                    'local_path': filepath\n",
    "                })\n",
    "        \n",
    "        response['downloaded_files'] = downloaded_files\n",
    "        response['status'] = 'success' if downloaded_files else 'partial'\n",
    "        response['message'] = f\"Found {len(urls)} URLs, downloaded {len(downloaded_files)} documents\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        response['message'] = str(e)\n",
    "    \n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    results = run_regulatory_search(\n",
    "        molecule=\"Paracetamol\",\n",
    "        experiment=\"prescription combination limits\",\n",
    "        region=\"India\"\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n=== RESULTS ===\")\n",
    "    print(f\"Status: {results['status']}\")\n",
    "    print(f\"Molecule: {results['molecule']}\")\n",
    "    print(f\"Experiment: {results['experiment']}\")\n",
    "    print(f\"Region: {results['region']}\")\n",
    "    \n",
    "    if results['downloaded_files']:\n",
    "        print(\"\\nDownloaded documents:\")\n",
    "        for file in results['downloaded_files']:\n",
    "            print(f\"- {file['local_path']} (from {file['url']})\")\n",
    "    \n",
    "    if results['found_urls']:\n",
    "        print(\"\\nAll found URLs:\")\n",
    "        for i, url in enumerate(results['found_urls'], 1):\n",
    "            print(f\"{i}. {url}\")\n",
    "    \n",
    "    print(f\"\\nMessage: {results['message']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9061d923",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Function must have a docstring if description not provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 147\u001b[39m\n\u001b[32m    142\u001b[39m     vs.save_local(VECTOR_DIR)\n\u001b[32m    144\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[38;5;66;03m# Tools for ReAct Agent\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m \u001b[38;5;129;43m@tool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_direct\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43mweb_search_tool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_tavily_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtitle\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtitle\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43murl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43murl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43murl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\PharmaReg\\multiagent_system\\env\\Lib\\site-packages\\langchain_core\\tools\\convert.py:266\u001b[39m, in \u001b[36mtool.<locals>._create_tool_factory.<locals>._tool_factory\u001b[39m\u001b[34m(dec_func)\u001b[39m\n\u001b[32m    263\u001b[39m     schema = args_schema\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m infer_schema \u001b[38;5;129;01mor\u001b[39;00m args_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStructuredTool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoroutine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_direct\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_direct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[43minfer_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_docstring\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_docstring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_on_invalid_docstring\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_on_invalid_docstring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;66;03m# If someone doesn't want a schema applied, we must treat it as\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[38;5;66;03m# a simple string->string function\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dec_func.\u001b[34m__doc__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\PharmaReg\\multiagent_system\\env\\Lib\\site-packages\\langchain_core\\tools\\structured.py:219\u001b[39m, in \u001b[36mStructuredTool.from_function\u001b[39m\u001b[34m(cls, func, coroutine, name, description, return_direct, args_schema, infer_schema, response_format, parse_docstring, error_on_invalid_docstring, **kwargs)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m description_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    218\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mFunction must have a docstring if description not provided.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m description \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# Only apply if using the function's docstring\u001b[39;00m\n\u001b[32m    222\u001b[39m     description_ = textwrap.dedent(description_).strip()\n",
      "\u001b[31mValueError\u001b[39m: Function must have a docstring if description not provided."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Advanced Regulatory + Reference Ingestion Agent with SSL Handling\n",
    "\n",
    "Includes:\n",
    "- ReAct agent using Tavily search (Web + PDF)\n",
    "- SSL certificate verification using certifi + fallback\n",
    "- WebBaseLoader + PyPDFLoader for URL ingestion\n",
    "- FAISS vector store + embeddings (OpenAI or HF)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import ssl\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import warnings\n",
    "import mimetypes\n",
    "import requests\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "import certifi\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Tavily imports\n",
    "try:\n",
    "    from langchain_tavily import TavilySearch\n",
    "    _TAVILY_MODE = \"new\"\n",
    "except ImportError:\n",
    "    from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "    _TAVILY_MODE = \"community\"\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "try:\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    _HAVE_OPENAI = True\n",
    "except ImportError:\n",
    "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "    _HAVE_OPENAI = False\n",
    "\n",
    "# -------------------------\n",
    "# Configuration\n",
    "# -------------------------\n",
    "DOWNLOAD_DIR = \"downloaded_docs\"\n",
    "VECTOR_DIR = \"vectorstore_faiss\"\n",
    "REQUEST_TIMEOUT = 40\n",
    "USER_AGENT = (\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "    \"(KHTML, like Gecko) Chrome/123.0 Safari/537.36\"\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# SSL-Safe Download\n",
    "# -------------------------\n",
    "def _download_ssl(url: str) -> Optional[str]:\n",
    "    os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "    fname = url.split(\"/\")[-1].split(\"?\")[0]\n",
    "    if not fname:\n",
    "        fname = hashlib.sha256(url.encode()).hexdigest()[:16]\n",
    "    filepath = os.path.join(DOWNLOAD_DIR, fname)\n",
    "\n",
    "    if os.path.exists(filepath):\n",
    "        return filepath\n",
    "\n",
    "    headers = {\"User-Agent\": USER_AGENT}\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT, verify=certifi.where())\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"[SSL] Cert verification failed for {url}: {e}, falling back to insecure...\")\n",
    "        warnings.simplefilter(\"ignore\", InsecureRequestWarning)\n",
    "        try:\n",
    "            resp = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT, verify=False)\n",
    "            resp.raise_for_status()\n",
    "        except Exception as e2:\n",
    "            print(f\"[SSL] Insecure fallback failed for {url}: {e2}\")\n",
    "            return None\n",
    "\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        for chunk in resp.iter_content(8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "    return filepath\n",
    "\n",
    "# -------------------------\n",
    "# Venue Search & Ingestion Functions\n",
    "# -------------------------\n",
    "def _tavily_search(query: str, max_results: int = 6) -> List[Dict[str, Any]]:\n",
    "    if _TAVILY_MODE == \"new\":\n",
    "        tool = TavilySearch(max_results=max_results, include_answer=False, include_raw_content=False)\n",
    "        out = tool.invoke({\"query\": query})\n",
    "        return out.get(\"results\", []) if isinstance(out, dict) else (out if isinstance(out, list) else [])\n",
    "    else:\n",
    "        tool = TavilySearchResults(max_results=max_results, include_answer=False, include_raw_content=False)\n",
    "        out = tool.invoke({\"query\": query})\n",
    "        if isinstance(out, list):\n",
    "            return [r if isinstance(r, dict) else {\"title\":\"\", \"url\":\"\", \"content\": str(r)} for r in out]\n",
    "        if isinstance(out, dict):\n",
    "            return [out]\n",
    "        return []\n",
    "\n",
    "def _load_url_docs(url: str) -> List[Dict[str, Any]]:\n",
    "    docs = []\n",
    "    try:\n",
    "        is_pdf = url.lower().endswith(\".pdf\")\n",
    "        local = _download_ssl(url)\n",
    "        if local and is_pdf:\n",
    "            loader = PyPDFLoader(local)\n",
    "        else:\n",
    "            loader = WebBaseLoader([url], header_template={\"User-Agent\": USER_AGENT}, continue_on_failure=True)\n",
    "        pages = loader.load()\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "        chunks = splitter.split_documents(pages)\n",
    "        for c in chunks:\n",
    "            docs.append({\"page_content\": c.page_content, \"metadata\": c.metadata})\n",
    "    except Exception as e:\n",
    "        print(f\"[load] Failed {url}: {e}\")\n",
    "    return docs\n",
    "\n",
    "def _get_embeddings():\n",
    "    if _HAVE_OPENAI and os.getenv(\"OPENAI_API_KEY\"):\n",
    "        return OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    return HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def _load_faiss():\n",
    "    embeddings = _get_embeddings()\n",
    "    if os.path.isdir(VECTOR_DIR):\n",
    "        try:\n",
    "            return FAISS.load_local(VECTOR_DIR, embeddings, allow_dangerous_deserialization=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def _save_faiss(vs: FAISS):\n",
    "    vs.save_local(VECTOR_DIR)\n",
    "\n",
    "# -------------------------\n",
    "# Tools for ReAct Agent\n",
    "# -------------------------\n",
    "@tool(\"web_search\", return_direct=True)\n",
    "def web_search_tool(query: str) -> str:\n",
    "    results = _tavily_search(query, max_results=6)\n",
    "    out = [{\"title\": r.get(\"title\",\"\"), \"url\": r.get(\"url\",\"\")} for r in results if r.get(\"url\")]\n",
    "    return json.dumps(out, ensure_ascii=False)\n",
    "\n",
    "@tool(\"ingest_url\", return_direct=True)\n",
    "def ingest_url_tool(url: str) -> str:\n",
    "    docs = _load_url_docs(url)\n",
    "    if not docs:\n",
    "        return json.dumps({\"url\": url, \"chunks\": 0})\n",
    "    embeddings = _get_embeddings()\n",
    "    vs = _load_faiss()\n",
    "    texts = [d[\"page_content\"] for d in docs]\n",
    "    metas = [d[\"metadata\"] for d in docs]\n",
    "    if vs is None:\n",
    "        vs = FAISS.from_texts(texts, embedding=embeddings, metadatas=metas)\n",
    "    else:\n",
    "        vs.add_texts(texts, metadatas=metas)\n",
    "    _save_faiss(vs)\n",
    "    return json.dumps({\"url\": url, \"chunks\": len(docs)})\n",
    "\n",
    "# -------------------------\n",
    "# Agent Setup\n",
    "# -------------------------\n",
    "AGENT_PROMPT = \"\"\"\n",
    "You are a Regulatory & Research Reference Agent for {region}. Your tasks:\n",
    "- Generate up to {max_q} precise queries to find both regulatory compliance documents and scientific references for the molecule.\n",
    "- Distinguish each found URL as \"compliance\" or \"reference\" with your reasoning.\n",
    "- For each reliable URL, call `ingest_url`.\n",
    "Finally, output JSON:\n",
    "{{\n",
    "  \"queries\": [...],\n",
    "  \"links\": [\n",
    "    {{\"url\":\"...\",\"title\":\"...\",\"type\":\"compliance\"|\"reference\",\"reason\":\"...\"}}\n",
    "  ],\n",
    "  \"note\":\"summary\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "def build_agent(region: str, max_q=6, max_links=10):\n",
    "    llm = ChatGroq(model=os.getenv(\"GROQ_MODEL\",\"gemma2-9b-it\"), temperature=0, api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    prompt = AGENT_PROMPT.format(region=region, max_q=max_q, max_links=max_links)\n",
    "    return create_react_agent(llm, [web_search_tool, ingest_url_tool], prompt=prompt)\n",
    "\n",
    "def run_full_pipeline(molecule: str, region: str, context: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    agent = build_agent(region)\n",
    "    sys = SystemMessage(content=\"Follow your system instructions precisely.\")\n",
    "    human = HumanMessage(content=json.dumps({\n",
    "        \"molecule\": molecule,\n",
    "        \"region\": region,\n",
    "        \"context\": context\n",
    "    }, ensure_ascii=False))\n",
    "    try:\n",
    "        resp_msg = agent.invoke({\"messages\":[sys, human]})\n",
    "        raw = getattr(resp_msg, \"content\", \"\") or resp_msg.get(\"output\", \"\")\n",
    "    except Exception as e:\n",
    "        return {\"status\":\"error\",\"message\":f\"Agent failed: {e}\"}\n",
    "    try:\n",
    "        return json.loads(raw)\n",
    "    except:\n",
    "        return {\"status\":\"partial\",\"message\":\"Could not parse JSON\",\"raw\": raw}\n",
    "\n",
    "# -------------------------\n",
    "# Example run\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    context ={\n",
    "        \"process_description\": \"The synthesis of Paracetamol involves a three-step process: 1) Acetylation, 2) Nitro reduction, and 3) Recrystallization.\",\n",
    "        \"specification\": \"The specifications for Paracetamol (IP 2022) include: - Assay: 99.0 \\u2013 101.0 % (HPLC with external standard). - Impurity A: \\u2264 0.10 % (HPLC). - Loss on Drying: \\u2264 0.5 % (USP <731>). - Residue on Ignition: \\u2264 0.1 % (USP <281>).\",\n",
    "        \"stability_report\": \"The stability study follows ICH Q1A (R2) guidelines. Accelerated conditions (40 \\u00b0C/75 % RH) for 6 months show: - Assay decrease: \\u22120.8 % (within specification). - Impurity A increase: +0.03 % (0.08 % at 6 months). Physical appearance remains unchanged. Long-term conditions (30 \\u00b0C/65 % RH) for 12 months show all parameters remain within specification. Paracetamol API is stable in LDPE/aluminium laminate packs with a proposed re-test period of 36 months when stored at \\u2264 30 \\u00b0C.\"\n",
    "    }\n",
    "    out = run_full_pipeline(\"Paracetamol\", \"India\", context)\n",
    "    print(json.dumps(out, indent=2, ensure_ascii=False))\n",
    "    print(\"Vector store path:\", VECTOR_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1f4adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load downloaded_docs\\document_1755519520.html: Invalid URL 'downloaded_docs\\\\document_1755519520.html': No scheme supplied. Perhaps you meant https://downloaded_docs\\document_1755519520.html?\n",
      "Failed to download https://www.researchgate.net/publication/376293608_Analysis_of_Various_Doses_of_Paracetamol_in_the_Indian_Market_A_Need_to_Revisit: 403 Client Error: Forbidden for url: https://www.researchgate.net/publication/376293608_Analysis_of_Various_Doses_of_Paracetamol_in_the_Indian_Market_A_Need_to_Revisit\n",
      "Failed to load downloaded_docs\\document_1755519520.html: Invalid URL 'downloaded_docs\\\\document_1755519520.html': No scheme supplied. Perhaps you meant https://downloaded_docs\\document_1755519520.html?\n",
      "Failed to load downloaded_docs\\cdsco.php.html: Invalid URL 'downloaded_docs\\\\cdsco.php.html': No scheme supplied. Perhaps you meant https://downloaded_docs\\cdsco.php.html?\n",
      "Failed to load downloaded_docs\\document_1755519521.html: Invalid URL 'downloaded_docs\\\\document_1755519521.html': No scheme supplied. Perhaps you meant https://downloaded_docs\\document_1755519521.html?\n",
      "Failed to load downloaded_docs\\7_5.pdf: `pypdf` package not found, please install it with `pip install pypdf`\n",
      "Failed to load downloaded_docs\\banneddrugs.pdf: `pypdf` package not found, please install it with `pip install pypdf`\n",
      "Failed to load downloaded_docs\\Import_guidance_doc.pdf: `pypdf` package not found, please install it with `pip install pypdf`\n",
      "Failed to load downloaded_docs\\31.10.23_20Draft_20National_20Pharmaceuticals_20Policy.pdf: `pypdf` package not found, please install it with `pip install pypdf`\n",
      "Failed to load downloaded_docs\\nlem2022.pdf: `pypdf` package not found, please install it with `pip install pypdf`\n",
      "\n",
      "=== RESULTS ===\n",
      "Status: error\n",
      "Molecule: Paracetamol\n",
      "Region: India\n",
      "\n",
      "Relevant URLs found:\n",
      "1. https://pmc.ncbi.nlm.nih.gov/articles/PMC9663334/\n",
      "2. https://www.researchgate.net/publication/376293608_Analysis_of_Various_Doses_of_Paracetamol_in_the_Indian_Market_A_Need_to_Revisit\n",
      "3. https://pmc.ncbi.nlm.nih.gov/articles/PMC11157186/\n",
      "4. https://dghs.mohfw.gov.in/cdsco.php\n",
      "5. https://cdsco.gov.in/opencms/opencms/en/Home/\n",
      "6. https://www.dsir.gov.in/sites/default/files/2019-11/7_5.pdf\n",
      "7. https://cdsco.gov.in/opencms/export/sites/CDSCO_WEB/Pdf-documents/Consumer_Section_PDFs/banneddrugs.pdf\n",
      "8. https://cdsco.gov.in/opencms/export/sites/CDSCO_WEB/Pdf-documents/import-registration/Import_guidance_doc.pdf\n",
      "9. https://pharma-dept.gov.in/sites/default/files/31.10.23%20Draft%20National%20Pharmaceuticals%20Policy.pdf\n",
      "10. https://cdsco.gov.in/opencms/resources/UploadCDSCOWeb/2018/UploadConsumer/nlem2022.pdf\n",
      "\n",
      "Message: No documents processed\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# Configuration\n",
    "DOWNLOAD_DIR = \"downloaded_docs\"\n",
    "VECTORSTORE_DIR = \"vectorstore\"\n",
    "REQUEST_TIMEOUT = 30\n",
    "MAX_DOWNLOADS = 10\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Domain whitelist\n",
    "TRUSTED_DOMAINS = {\n",
    "    'india': ['cdsco.gov.in', 'ipc.gov.in', 'pharmaceuticals.gov.in', 'who.int', \n",
    "              'ich.org', 'ncbi.nlm.nih.gov', 'researchgate.net', 'sciencedirect.com',\n",
    "              'gov.in', 'indianjournals.com', 'pharmatutor.org'],\n",
    "    'default': ['fda.gov', 'ema.europa.eu', 'who.int', 'ich.org', 'ncbi.nlm.nih.gov']\n",
    "}\n",
    "\n",
    "def initialize_environment():\n",
    "    \"\"\"Create necessary directories.\"\"\"\n",
    "    os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "    os.makedirs(VECTORSTORE_DIR, exist_ok=True)\n",
    "\n",
    "def get_search_tool():\n",
    "    \"\"\"Initialize Tavily search tool with proper configuration.\"\"\"\n",
    "    tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "    if not tavily_api_key:\n",
    "        raise ValueError(\"TAVILY_API_KEY not set in environment variables\")\n",
    "    return TavilySearchResults(max_results=5, include_raw_content=True)\n",
    "\n",
    "def get_embeddings() -> Embeddings:\n",
    "    \"\"\"Initialize embeddings model.\"\"\"\n",
    "    return HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def is_relevant_url(url: str, region: str) -> bool:\n",
    "    \"\"\"Check if URL is from a trusted domain.\"\"\"\n",
    "    try:\n",
    "        domain = urlparse(url).netloc.lower()\n",
    "        domain = re.sub(r'^www\\.', '', domain)\n",
    "        trusted_domains = TRUSTED_DOMAINS.get(region.lower(), TRUSTED_DOMAINS['default'])\n",
    "        return any(domain.endswith(td) for td in trusted_domains)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def sanitize_filename(filename: str) -> str:\n",
    "    \"\"\"Sanitize filename to be filesystem-safe.\"\"\"\n",
    "    return re.sub(r'[^\\w\\-_. ]', '_', filename)\n",
    "\n",
    "def download_content(url: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"Download content from URL and save locally.\"\"\"\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        if not all([parsed.scheme, parsed.netloc]):\n",
    "            raise ValueError(f\"Invalid URL: {url}\")\n",
    "\n",
    "        filename = sanitize_filename(os.path.basename(parsed.path)) or f\"document_{int(time.time())}\"\n",
    "        filepath = os.path.join(DOWNLOAD_DIR, filename)\n",
    "        \n",
    "        # Check existing files to avoid duplicates\n",
    "        if os.path.exists(filepath):\n",
    "            return filepath, None\n",
    "            \n",
    "        response = requests.get(url, headers=HEADERS, stream=True, timeout=REQUEST_TIMEOUT)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Determine content type and extension\n",
    "        content_type = response.headers.get('Content-Type', '')\n",
    "        if 'application/pdf' in content_type:\n",
    "            if not filepath.lower().endswith('.pdf'):\n",
    "                filepath += '.pdf'\n",
    "        elif not filepath.lower().endswith(('.html', '.htm')):\n",
    "            filepath += '.html'\n",
    "        \n",
    "        # Save content\n",
    "        with open(filepath, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        \n",
    "        return filepath, content_type\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {url}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def load_document(filepath: str, content_type: str = None) -> Optional[List]:\n",
    "    \"\"\"Load document based on its type.\"\"\"\n",
    "    try:\n",
    "        if filepath.lower().endswith('.pdf') or (content_type and 'pdf' in content_type.lower()):\n",
    "            return PyPDFLoader(filepath).load()\n",
    "        else:\n",
    "            return WebBaseLoader(filepath).load()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {filepath}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def generate_search_queries(molecule: str, context: Dict, region: str) -> List[str]:\n",
    "    \"\"\"Generate focused search queries using LLM.\"\"\"\n",
    "    llm = ChatGroq(\n",
    "        model=\"deepseek-r1-distill-llama-70b\",\n",
    "        temperature=0.2,\n",
    "        api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "    )\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a pharmaceutical regulatory expert. Generate specific search queries to find:\n",
    "        1. Regulatory compliance documents (official guidelines, pharmacopoeia standards)\n",
    "        2. Scientific reference documents (research papers, clinical studies)\n",
    "        \n",
    "        For molecule: {molecule} in region: {region}\n",
    "        Context: {context}\n",
    "        \n",
    "        Requirements:\n",
    "        - Focus on official sources (government, regulatory bodies)\n",
    "        - Prioritize PDF documents\n",
    "        - Include site-specific searches for {region} domains\n",
    "        - Exclude news articles and commercial websites\n",
    "        \n",
    "        Return ONLY the search queries, one per line.\"\"\"),\n",
    "        (\"human\", \"Generate search queries for {molecule} in {region}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    response = chain.invoke({\n",
    "        \"molecule\": molecule,\n",
    "        \"context\": \"\\n\".join(f\"{k}: {v}\" for k,v in context.items()),\n",
    "        \"region\": region\n",
    "    })\n",
    "    \n",
    "    return [q.strip() for q in response.split('\\n') if q.strip()]\n",
    "\n",
    "def run_regulatory_search(molecule: str, context: Dict, region: str) -> Dict:\n",
    "    \"\"\"Main search and document processing workflow.\"\"\"\n",
    "    initialize_environment()\n",
    "    response = {\n",
    "        'status': 'error',\n",
    "        'molecule': molecule,\n",
    "        'region': region,\n",
    "        'context': context,\n",
    "        'found_urls': [],\n",
    "        'downloaded_files': [],\n",
    "        'vectorstore': None,\n",
    "        'message': 'Initialization failed'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Generate search queries\n",
    "        queries = generate_search_queries(molecule, context, region)\n",
    "        if not queries:\n",
    "            response['message'] = \"No search queries generated\"\n",
    "            return response\n",
    "        \n",
    "        # Step 2: Execute searches\n",
    "        search_tool = get_search_tool()\n",
    "        all_results = []\n",
    "        for query in queries[:5]:  # Limit to 5 queries\n",
    "            try:\n",
    "                results = search_tool.invoke({\"query\": query})\n",
    "                if isinstance(results, list):\n",
    "                    all_results.extend(results)\n",
    "                elif isinstance(results, dict) and 'results' in results:\n",
    "                    all_results.extend(results['results'])\n",
    "            except Exception as e:\n",
    "                print(f\"Search failed for '{query}': {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not all_results:\n",
    "            response['message'] = \"No search results found\"\n",
    "            return response\n",
    "        \n",
    "        # Step 3: Filter and process URLs\n",
    "        urls = []\n",
    "        for result in all_results:\n",
    "            if isinstance(result, dict):\n",
    "                url = result.get('url')\n",
    "                if url and is_relevant_url(url, region):\n",
    "                    urls.append(url)\n",
    "        \n",
    "        response['found_urls'] = urls\n",
    "        \n",
    "        # Step 4: Download and process content\n",
    "        downloaded_files = []\n",
    "        all_docs = []\n",
    "        embeddings = get_embeddings()\n",
    "        \n",
    "        for url in urls[:MAX_DOWNLOADS]:\n",
    "            try:\n",
    "                filepath, content_type = download_content(url)\n",
    "                if filepath:\n",
    "                    docs = load_document(filepath, content_type)\n",
    "                    if docs:\n",
    "                        downloaded_files.append({\n",
    "                            'url': url,\n",
    "                            'local_path': filepath\n",
    "                        })\n",
    "                        all_docs.extend(docs)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not all_docs:\n",
    "            response['message'] = \"No documents processed\"\n",
    "            return response\n",
    "        \n",
    "        # Step 5: Create vectorstore\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        splits = text_splitter.split_documents(all_docs)\n",
    "        \n",
    "        vectorstore_name = f\"{molecule.lower()}_{region.lower()}_docs\".replace(' ', '_')\n",
    "        vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "        vectorstore.save_local(os.path.join(VECTORSTORE_DIR, vectorstore_name))\n",
    "        \n",
    "        response.update({\n",
    "            'downloaded_files': downloaded_files,\n",
    "            'vectorstore': os.path.join(VECTORSTORE_DIR, vectorstore_name),\n",
    "            'status': 'success',\n",
    "            'message': f\"Processed {len(downloaded_files)} documents. Vectorstore created.\"\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        response['message'] = str(e)\n",
    "    \n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    context = {\n",
    "        \"process_description\": \"The synthesis of Paracetamol involves a three-step process: 1) Acetylation, 2) Nitro reduction, and 3) Recrystallization.\",\n",
    "        \"specification\": \"The specifications for Paracetamol (IP 2022) include: - Assay: 99.0 \\u2013 101.0 % (HPLC with external standard). - Impurity A: \\u2264 0.10 % (HPLC). - Loss on Drying: \\u2264 0.5 % (USP <731>). - Residue on Ignition: \\u2264 0.1 % (USP <281>).\",\n",
    "        \"stability_report\": \"The stability study follows ICH Q1A (R2) guidelines. Accelerated conditions (40 \\u00b0C/75 % RH) for 6 months show: - Assay decrease: \\u22120.8 % (within specification). - Impurity A increase: +0.03 % (0.08 % at 6 months). Physical appearance remains unchanged. Long-term conditions (30 \\u00b0C/65 % RH) for 12 months show all parameters remain within specification. Paracetamol API is stable in LDPE/aluminium laminate packs with a proposed re-test period of 36 months when stored at \\u2264 30 \\u00b0C.\"\n",
    "    }\n",
    "    \n",
    "    results = run_regulatory_search(\n",
    "        molecule=\"Paracetamol\",\n",
    "        context=context,\n",
    "        region=\"India\"\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n=== RESULTS ===\")\n",
    "    print(f\"Status: {results['status']}\")\n",
    "    print(f\"Molecule: {results['molecule']}\")\n",
    "    print(f\"Region: {results['region']}\")\n",
    "    \n",
    "    if results['downloaded_files']:\n",
    "        print(\"\\nDownloaded documents:\")\n",
    "        for file in results['downloaded_files']:\n",
    "            print(f\"- {file['local_path']} (from {file['url']})\")\n",
    "    \n",
    "    if results['found_urls']:\n",
    "        print(\"\\nRelevant URLs found:\")\n",
    "        for i, url in enumerate(results['found_urls'], 1):\n",
    "            print(f\"{i}. {url}\")\n",
    "    \n",
    "    if results['vectorstore']:\n",
    "        print(f\"\\nVectorstore created at: {results['vectorstore']}\")\n",
    "    \n",
    "    print(f\"\\nMessage: {results['message']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
