{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6060cc92",
   "metadata": {},
   "source": [
    "Lec 25 :- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5892e659",
   "metadata": {},
   "source": [
    "Neural Network Architecture\n",
    "1) #Neuron# is the basic component or smallest unit of the neural network which takes the input, process it in its given equation or work given to it and then provides the output (building blocks)\n",
    "input -> neuron -> output\n",
    "\n",
    "[ like our biological neural network works -> it takes an input from its previous neuron then process it ( according to it works if any signal is there for it ) -> then it pass it to the brain -> brain process it and pass signal to the neuron as it wants ]\n",
    "(each input will be assigned a weight[importance])\n",
    "x1*w1   \\\n",
    "x2*w2  -- y => summation \n",
    "x3*w3   /\n",
    "(x1,x2,x3 are inputs and w1,w2 and w3 are weights)\n",
    "#135 billion neurons in human body\n",
    "input ----> 1) summation[weighted sum of inputs] 2) add bias 3) pass it through activation function (for decision making of whether we have to fired the neuron or not by verifying the threshold value)\n",
    "it assignes the weight by itself by first keeping it random bydefault and then updating it with try and error method with measuring the loss using loss function\n",
    "weights(importance)\n",
    "\n",
    "x1.w1 + x2.w2 + x3.w3 -->>>     x1.w1 + x2.w2 + x3.w3 + b      -->>>    f(x1.w1 + x2.w2 + x3.w3 +b)\n",
    "activation function ->>> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f1bac9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88cfc9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x=np.array([1,0])\n",
    "w=np.array([1,1])\n",
    "b=np.array([0])\n",
    "y=np.dot(x,w)+b\n",
    "if y>=2:\n",
    "    print(1)\n",
    "else:   \n",
    "    print(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56e06589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron output: 0.9996293938593735\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# inputs: study hours, sleep hours\n",
    "X = np.array([6, 7])\n",
    "\n",
    "# weights\n",
    "W = np.array([0.8, 0.3])\n",
    "\n",
    "# bias\n",
    "b = 1.0\n",
    "\n",
    "# weighted sum\n",
    "z = np.dot(W, X) + b\n",
    "\n",
    "# activation\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "output = sigmoid(z)\n",
    "\n",
    "print(\"Neuron output:\", output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "975b01d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer output: [5.4 6.  7.3]\n"
     ]
    }
   ],
   "source": [
    "# 3 neurons, 2 inputs\n",
    "W = np.array([\n",
    "    [0.5, 0.2],\n",
    "    [0.1, 0.7],\n",
    "    [0.8, 0.4]\n",
    "])\n",
    "\n",
    "b = np.array([1.0, 0.5, -0.3])\n",
    "\n",
    "z = np.dot(W, X) + b\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "A = relu(z)\n",
    "\n",
    "print(\"Layer output:\", A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c059223a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prediction: [0.99620398]\n"
     ]
    }
   ],
   "source": [
    "# Output layer (1 neuron)\n",
    "W2 = np.array([[0.6, 0.2, 0.1]])\n",
    "b2 = np.array([0.4])\n",
    "\n",
    "z2 = np.dot(W2, A) + b2\n",
    "output = sigmoid(z2)\n",
    "\n",
    "print(\"Final prediction:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77292e51",
   "metadata": {},
   "source": [
    "What PyTorch does automatically\n",
    "\n",
    "creates weights\n",
    "\n",
    "creates bias\n",
    "\n",
    "handles matrix math\n",
    "\n",
    "supports backpropagation\n",
    "\n",
    "But conceptually, it is doing:\n",
    "\n",
    "WX+b→activation\n",
    "\n",
    "Same math. Cleaner code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49adf691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch\n",
      "  Using cached pytorch-1.0.2.tar.gz (689 bytes)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: pytorch\n",
      "  Building wheel for pytorch (pyproject.toml): started\n",
      "  Building wheel for pytorch (pyproject.toml): finished with status 'error'\n",
      "Failed to build pytorch\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for pytorch (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [37 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \u001b[35m\"c:\\Users\\Vishaka\\miniconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "          \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "          \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "        File \u001b[35m\"c:\\Users\\Vishaka\\miniconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "          json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
      "                                   \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"c:\\Users\\Vishaka\\miniconda3\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m280\u001b[0m, in \u001b[35mbuild_wheel\u001b[0m\n",
      "          return \u001b[31m_build_backend().build_wheel\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                 \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "              \u001b[1;31mwheel_directory, config_settings, metadata_directory\u001b[0m\n",
      "              \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "          \u001b[1;31m)\u001b[0m\n",
      "          \u001b[1;31m^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Vishaka\\AppData\\Local\\Temp\\pip-build-env-5q9oxehu\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m439\u001b[0m, in \u001b[35mbuild_wheel\u001b[0m\n",
      "          return _build(['bdist_wheel', '--dist-info-dir', str(metadata_directory)])\n",
      "        File \u001b[35m\"C:\\Users\\Vishaka\\AppData\\Local\\Temp\\pip-build-env-5q9oxehu\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m427\u001b[0m, in \u001b[35m_build\u001b[0m\n",
      "          return \u001b[31mself._build_with_temp_dir\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                 \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "              \u001b[1;31mcmd,\u001b[0m\n",
      "              \u001b[1;31m^^^^\u001b[0m\n",
      "          ...<3 lines>...\n",
      "              \u001b[1;31mself._arbitrary_args(config_settings),\u001b[0m\n",
      "              \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "          \u001b[1;31m)\u001b[0m\n",
      "          \u001b[1;31m^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Vishaka\\AppData\\Local\\Temp\\pip-build-env-5q9oxehu\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m408\u001b[0m, in \u001b[35m_build_with_temp_dir\u001b[0m\n",
      "          \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Vishaka\\AppData\\Local\\Temp\\pip-build-env-5q9oxehu\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m518\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "          \u001b[31msuper().run_setup\u001b[0m\u001b[1;31m(setup_script=setup_script)\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Vishaka\\AppData\\Local\\Temp\\pip-build-env-5q9oxehu\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m317\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "          \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
      "          \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m15\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "      \u001b[1;35mException\u001b[0m: \u001b[35mYou tried to install \"pytorch\". The package named for PyTorch is \"torch\"\u001b[0m\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for pytorch\n",
      "error: failed-wheel-build-for-install\n",
      "\n",
      "× Failed to build installable wheels for some pyproject.toml based projects\n",
      "╰─> pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6001], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "%pip install pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(2, 3)  # 2 inputs → 3 neurons\n",
    "        self.output = nn.Linear(3, 1)  # 3 → 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "model = SimpleNN()\n",
    "\n",
    "# input tensor\n",
    "x = torch.tensor([6.0, 7.0])\n",
    "\n",
    "prediction = model(x)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3a223f",
   "metadata": {},
   "source": [
    "NEW START\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efd4d689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8320183851339245\n",
      "Dog\n"
     ]
    }
   ],
   "source": [
    "inputs=[1,2,4] #pupil, ear, whisker\n",
    "weights=[0.1,0.5,0.4]\n",
    "bias=0.0\n",
    "for i in range(len(inputs)):\n",
    "    weighted_inputs=inputs[i]*weights[i]\n",
    "ans=weighted_inputs+bias\n",
    "\n",
    "import math\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))\n",
    "\n",
    "result=sigmoid(ans)\n",
    "print(result)    \n",
    "\n",
    "if result>0.5:\n",
    "    print(\"Dog\")\n",
    "else :\n",
    "    print(\"Cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "737ba66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93702664]\n",
      "Dog\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x=np.array([1,2,4]) #pupil, ear, whisker\n",
    "w=np.array([0.1,0.5,0.4])\n",
    "b=np.array([0.0])\n",
    "ans=np.dot(x,w)+b\n",
    "def sigmoid(x): \n",
    "    return 1/(1+np.exp(-x))\n",
    "result=sigmoid(ans)\n",
    "print(result)\n",
    "if result>0.5:\n",
    "    print(\"Dog\")    \n",
    "else :\n",
    "    print(\"Cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d35f3b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Prediction: 0.51\n",
      "Error: 0.49\n",
      "Weight 0 changed by 0.0005\n",
      "Weight 1 changed by 0.0005\n",
      "Weight 2 changed by 0.0490\n"
     ]
    }
   ],
   "source": [
    "# 1. Inputs (Previous layer's output)\n",
    "# Notice x3 is much larger (The \"Whisker Length\" example from lecture [4])\n",
    "inputs = [0.01, 0.01, 1.0]\n",
    "\n",
    "# 2. Current Weights (Randomly initialized)\n",
    "weights = [0.5, 0.5, 0.5] \n",
    "\n",
    "# 3. Target Output (1.0 for Cat)\n",
    "target = 1.0\n",
    "\n",
    "# 4. Forward Pass (Prediction)\n",
    "def predict(inputs, weights):\n",
    "    # Simple Dot Product\n",
    "    output = 0\n",
    "    for i in range(len(inputs)):\n",
    "        output += inputs[i] * weights[i]\n",
    "    return output\n",
    "\n",
    "current_prediction = predict(inputs, weights)\n",
    "print(f\"Current Prediction: {current_prediction}\") # Output: 0.51\n",
    "\n",
    "# 5. Calculate Error (Simple difference)\n",
    "error = target - current_prediction\n",
    "print(f\"Error: {error}\") # Output: 0.49 (We need to increase output)\n",
    "\n",
    "# 6. Backpropagation Step (The Nudge)\n",
    "learning_rate = 0.1 # How big of a step we take\n",
    "\n",
    "new_weights = []\n",
    "for i in range(len(weights)):\n",
    "    # KEY INTUITION: We update weight proportional to the INPUT [4]\n",
    "    # If input is high, this weight contributed more to the error.\n",
    "    weight_change = error * inputs[i] * learning_rate\n",
    "    \n",
    "    new_weights.append(weights[i] + weight_change)\n",
    "    print(f\"Weight {i} changed by {weight_change:.4f}\")\n",
    "\n",
    "# Notice Weight 2 (connected to 1.0) changes significantly more \n",
    "# than Weight 0 or 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071e1d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\vishaka\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\vishaka\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\vishaka\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\vishaka\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\vishaka\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vishaka\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\vishaka\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (2026.1.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vishaka\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from torch) (80.10.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vishaka\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vishaka\\appdata\\local\\programs\\python\\python314\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Prediction: 0.6248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to Tensor.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     21\u001b[39m loss.backward() \n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# PyTorch just calculated the gradients for us!\u001b[39;00m\n\u001b[32m     23\u001b[39m \n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# 6. Check the \"Sensitivity\" (Gradients)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGradient for W1 (Input 0.01): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw.grad\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGradient for W3 (Input 1.00): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw.grad[\u001b[32m10\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# You will see W3 has a MUCH larger gradient. \u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# It is 100x more important to update W3 than W1 [7].\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vishaka\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch\\_tensor.py:1151\u001b[39m, in \u001b[36mTensor.__format__\u001b[39m\u001b[34m(self, format_spec)\u001b[39m\n\u001b[32m   1147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dim() == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[32m   1148\u001b[39m     \u001b[38;5;66;03m# Use detach() here to avoid the warning when converting a scalar Tensor that\u001b[39;00m\n\u001b[32m   1149\u001b[39m     \u001b[38;5;66;03m# requires gradients to a python number. It is ok for formatting.\u001b[39;00m\n\u001b[32m   1150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.detach().item().\u001b[34m__format__\u001b[39m(format_spec)\n\u001b[32m-> \u001b[39m\u001b[32m1151\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__format__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_spec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: unsupported format string passed to Tensor.__format__"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "import torch\n",
    "\n",
    "# 1. Define Inputs and Weights (Requires_grad=True tracks the history for backprop)\n",
    "x = torch.tensor([0.01, 0.01, 1.0])\n",
    "w = torch.tensor([0.5, 0.5, 0.5], requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# 2. Target\n",
    "target = torch.tensor(1.0)\n",
    "\n",
    "# 3. Forward Pass\n",
    "# This builds a computation graph in the background\n",
    "prediction = torch.sigmoid(torch.dot(x, w) + b)\n",
    "print(f\"Prediction: {prediction.item():.4f}\")\n",
    "\n",
    "# 4. Calculate Loss (Mean Squared Error)\n",
    "loss = (prediction - target)**2\n",
    "\n",
    "# 5. Backward Pass (The Magic)\n",
    "loss.backward() \n",
    "# PyTorch just calculated the gradients for us!\n",
    "\n",
    "# 6. Check the \"Sensitivity\" (Gradients)\n",
    "# 6. Check the \"Sensitivity\" (Gradients)\n",
    "print(f\"Gradient for W1 (Input 0.01): {w.grad[0].item():.6f}\")\n",
    "print(f\"Gradient for W2 (Input 0.01): {w.grad[1].item():.6f}\")\n",
    "print(f\"Gradient for W3 (Input 1.00): {w.grad[2].item():.6f}\")\n",
    "\n",
    "\n",
    "# You will see W3 has a MUCH larger gradient. \n",
    "# It is 100x more important to update W3 than W1 [7]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514e8539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
